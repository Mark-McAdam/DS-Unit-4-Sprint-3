{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "JC_Lecture_DS_431_RNN_and_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ldr0HZ193GKb"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 3, Module 1*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSsErpK3ZGS2",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
        "<br></br>\n",
        "<br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA6zSVI-ZGS4",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IizNKWLomoA"
      },
      "source": [
        "## Overview\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44QZgrPUe3-Y"
      },
      "source": [
        "# Neural Networks for Sequences (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6v3D8uoZGS_"
      },
      "source": [
        "## Overview\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrQllf8WEd-"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Evqf0s1JZGTB"
      },
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ti23G0gRe3kr",
        "outputId": "f83f8f44-10f9-4e15-bc4f-61d706b75372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiBzgUjLZGTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IXCbjxhZGTM",
        "colab_type": "code",
        "outputId": "5f054003-9fe5-4098-8604-9c4c170e92b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Pad Sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad Sequences (samples x time)\n",
            "x_train shape:  (25000, 80)\n",
            "x_test shape:  (25000, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHeHwdreZGTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCUjM8RUZGTU",
        "colab_type": "code",
        "outputId": "a598dd9f-7122-4429-a50c-dd5b49aa8026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxVyO_WoZGTY",
        "colab_type": "code",
        "outputId": "a5cdc911-f505-42ff-899e-1a786dbf1445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "unicorns = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size, \n",
        "          epochs=5, \n",
        "          validation_data=(x_test,y_test))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 393s 503ms/step - loss: 0.4355 - accuracy: 0.7954 - val_loss: 0.3739 - val_accuracy: 0.8371\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 391s 500ms/step - loss: 0.2574 - accuracy: 0.8966 - val_loss: 0.3826 - val_accuracy: 0.8260\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 381s 488ms/step - loss: 0.1676 - accuracy: 0.9362 - val_loss: 0.4654 - val_accuracy: 0.8296\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 386s 494ms/step - loss: 0.1097 - accuracy: 0.9600 - val_loss: 0.5594 - val_accuracy: 0.8257\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 374s 478ms/step - loss: 0.0729 - accuracy: 0.9748 - val_loss: 0.7345 - val_accuracy: 0.8188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SIPulZGZGTc",
        "colab_type": "code",
        "outputId": "d4abd13f-c1b2-42de-b396-42dd81e9b304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dBd5V47ZGTf",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pETWPIe362y"
      },
      "source": [
        "# LSTM Text generation with Keras (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CTYilIS5ZGTg"
      },
      "source": [
        "## Overview\n",
        "\n",
        "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
        "\n",
        "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onWHakEBZGTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g5bq9HxZGTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_json(\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/master/module1-rnn-and-lstm/wp_articles.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DI23NAznInw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data['article'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWlPeWMsZGTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in Data\n",
        "\n",
        "data = []\n",
        "\n",
        "for file in data_files:\n",
        "    if file[-3:] == 'txt':\n",
        "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
        "            data.append(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95a1GYhtZGTt",
        "colab_type": "code",
        "outputId": "50e1ff51-5afa-4586-e5f6-383b7e9bc6ea",
        "colab": {}
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mILzOGdBZGTy",
        "colab_type": "code",
        "outputId": "5cec0c77-2a18-458f-94df-d6cfacb26138",
        "colab": {}
      },
      "source": [
        "data[-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here are some recent headlines from schools around the country: In Indiana, officials played a segment of a 911 call of a teacher in a panic during the Columbine High School shooting to students. In Ohio, officers fired blank shots during an active-shooter drill. In South Carolina, an officer dressed in black posed as an intruder on an unannounced drill. In Michigan, a school is spending $48 million on a renovation that includes curved hallways and hiding niches, in hopes of protecting students from a mass shooting. In Florida, a police officer arrested two 6-year-old students for misdemeanor battery. In Colorado, teachers received buckets and kitty litter for students to use as toilets in case of a prolonged school lockdown.\\n\\nMass shootings, meaning incidents with at least two deaths, in schools are horrifying. But it is highly unlikely that a child would ever witness one. Research indicates that some security measures brought in to make schools safer — like realistic shooter trainings — may be causing children more harm than good.\\n\\nIt is 10 times more likely that a student will die on the way to school.\\n\\nOur chances of dying in a fire are also much greater — 1 in 1,500. But we don’t overreact.\\n\\nMore children have died from lightning strikes than from mass shootings in schools in the past 20 years. Still, we don’t obsess about them.\\n\\nExactly how common are school shootings?\\n\\nIn the two decades since Columbine, there have been 10 mass shootings in schools according to a recent analysis by James Alan Fox, a professor of criminology at Northeastern University who has been studying school violence for several decades. In total, 81 people have been killed, 64 of them students. That’s an average of four deaths per year, three of them students.\\n\\nEven one death is too many. But for perspective, 729 children committed suicide with a firearm in 2017, and 863 were victims of homicides by guns that year.\\n\\nSchool-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides in 2017 Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University.\\n\\nNearly every public school in the country now conducts lockdown drills, and even the youngest students participate (last year, one school adapted a lullaby to prepare kindergartners). But very few studies have looked into the efficiency of these drills. One of them concluded that the practice can be helpful to teach students basic safety procedures. But to the author of the study, Jaclyn Schildkraut, an associate professor at the State University of New York at Oswego, there is no point in dramatizing the drills. “All that causes is fear,” she said.\\n\\nRestaurants have 10 times as many homicides as schools. Why do we want to arm teachers and not wait staffs?\\n\\n“There’s a misunderstanding in where the dangers are,” said Dewey G. Cornell, a psychologist and professor at the University of Virginia. “Kids are at far greater danger going to and from school, than they are in the classroom,” he said. “School counseling, academic support, that’s gonna do far more to keep our communities safe.”\\n\\nUnlike the United States, the other wealthy countries in the Group of Seven don’t do lockdown drills and rarely have school shootings. What is the United States doing that is so different from them?\\n\\nGun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017.\\n\\nMany researchers think easy access to guns is an important part of the problem. “Violence in schools is just a small part of the larger problem of gun violence in our society,” Cornell wrote in a statement about prevention of violence in schools and communities.\\n\\nMisguided safety measures, such as dramatized lockdown drills, may give us the impression that we are protecting children, when, in fact, we are handing them a burden that adults are failing to address.\\n\\nRead more:\\n\\n‘What if someone was shooting?’\\n\\nThey grew up practicing lockdown drills. Now they’re steering the conversation on gun violence.\\n\\nSchool shootings are extraordinarily rare. Why is fear of them driving policy?\\n\\nPutting more cops in schools won’t make schools safer, and it will likely inflict a lot of harm'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba2Zf3hvZGT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode Data as Chars\n",
        "\n",
        "# Gather all text \n",
        "# Why? 1. See all possible characters 2. For training / splitting later\n",
        "text = \" \".join(data)\n",
        "\n",
        "# Unique Characters\n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi9x01lJZGT6",
        "colab_type": "code",
        "outputId": "09aad17a-00db-4d95-849c-cc6e320eb768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtLGmWBtoESA",
        "colab_type": "code",
        "outputId": "244146b9-c0a2-4aec-e7ac-4305327209f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "chars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J',\n",
              " '_',\n",
              " 'G',\n",
              " '\"',\n",
              " 'c',\n",
              " '⅔',\n",
              " 'Q',\n",
              " 'k',\n",
              " 'á',\n",
              " '!',\n",
              " 'D',\n",
              " '$',\n",
              " '🤔',\n",
              " '2',\n",
              " '\\u2066',\n",
              " 'T',\n",
              " '…',\n",
              " 'C',\n",
              " 'Y',\n",
              " ',',\n",
              " '•',\n",
              " \"'\",\n",
              " 'F',\n",
              " 'h',\n",
              " '9',\n",
              " 'R',\n",
              " '0',\n",
              " '|',\n",
              " '%',\n",
              " 'P',\n",
              " 'ö',\n",
              " '-',\n",
              " 'H',\n",
              " '\\xad',\n",
              " '&',\n",
              " 'j',\n",
              " '”',\n",
              " 'y',\n",
              " '–',\n",
              " ')',\n",
              " 'W',\n",
              " '🗣',\n",
              " 'm',\n",
              " 'Z',\n",
              " 'L',\n",
              " 'a',\n",
              " 'X',\n",
              " '+',\n",
              " 'U',\n",
              " 'N',\n",
              " '👻',\n",
              " 'ﬂ',\n",
              " '{',\n",
              " ':',\n",
              " '\\u2069',\n",
              " 'o',\n",
              " '3',\n",
              " 'O',\n",
              " '“',\n",
              " 'n',\n",
              " 'q',\n",
              " 'é',\n",
              " '#',\n",
              " 'u',\n",
              " 'g',\n",
              " '?',\n",
              " '×',\n",
              " 'r',\n",
              " 'í',\n",
              " '―',\n",
              " 'K',\n",
              " '\\n',\n",
              " '©',\n",
              " 'b',\n",
              " '@',\n",
              " 'v',\n",
              " 'ã',\n",
              " '.',\n",
              " '7',\n",
              " '⅓',\n",
              " 'p',\n",
              " '(',\n",
              " 'e',\n",
              " '1',\n",
              " 'E',\n",
              " '½',\n",
              " 'f',\n",
              " 'M',\n",
              " '6',\n",
              " 'd',\n",
              " '4',\n",
              " 'ñ',\n",
              " 'ó',\n",
              " 'ê',\n",
              " '‘',\n",
              " 'I',\n",
              " 't',\n",
              " '—',\n",
              " ' ',\n",
              " '5',\n",
              " 'z',\n",
              " 'A',\n",
              " '*',\n",
              " '[',\n",
              " 'w',\n",
              " ';',\n",
              " 's',\n",
              " '’',\n",
              " 'l',\n",
              " ']',\n",
              " 'V',\n",
              " 'S',\n",
              " 'x',\n",
              " '·',\n",
              " '8',\n",
              " 'i',\n",
              " 'B',\n",
              " '●',\n",
              " '⭐',\n",
              " 'è',\n",
              " '/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRddMJtXZGT9",
        "colab_type": "code",
        "outputId": "f05aaaa8-418a-4246-b1e5-242c6dee6933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create the sequence data\n",
        "\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  178374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwUB69BsZGUB",
        "colab_type": "code",
        "outputId": "7e6ad56d-6987-4f35-fdc0-34055e5652f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "sequences[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17,\n",
              " 55,\n",
              " 59,\n",
              " 96,\n",
              " 67,\n",
              " 115,\n",
              " 73,\n",
              " 63,\n",
              " 96,\n",
              " 115,\n",
              " 59,\n",
              " 64,\n",
              " 98,\n",
              " 4,\n",
              " 55,\n",
              " 108,\n",
              " 63,\n",
              " 42,\n",
              " 59,\n",
              " 115,\n",
              " 106,\n",
              " 96,\n",
              " 71,\n",
              " 71,\n",
              " 15,\n",
              " 23,\n",
              " 82,\n",
              " 98,\n",
              " 32,\n",
              " 55,\n",
              " 63,\n",
              " 106,\n",
              " 82,\n",
              " 98,\n",
              " 115,\n",
              " 106,\n",
              " 98,\n",
              " 55,\n",
              " 59,\n",
              " 98]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS7gWa88ZGUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create x & y\n",
        "\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9h9tGMsZGUJ",
        "colab_type": "code",
        "outputId": "1ce9f4cd-197b-44ca-c19a-bd9dfd42acda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178374, 40, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqoBqHqLZGUL",
        "colab_type": "code",
        "outputId": "49c11136-98e7-4dd9-ff5c-213f6e045172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178374, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVMkWiz5rnLs",
        "colab_type": "code",
        "outputId": "752c3d43-cb4d-4046-b4d1-05973d14f93b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "x[0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False,  True,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTIt5q5ZGUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='nadam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn7zT1INZGUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm5JIutbZGUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts6ZcUgIZGUX",
        "colab_type": "code",
        "outputId": "04d18a3a-43bc-4460-f0ee-e209b5386e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=256,\n",
        "          epochs=10,\n",
        "          validation_split=.2,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "558/558 [==============================] - ETA: 0s - loss: 2.0514\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"ene. By 4:15 p.m., they said there was “\"\n",
            "ene. By 4:15 p.m., they said there was “Ala suss rolgited on sabdon. Strammad. Thes waling by an the U.S/AS and the ferteres conlais the caid disting in promed ot sho meal on Sorkitica. 15 net changet ralokses for “uloplana ally and hit candly in plicussers, his ores nat mant efripod and Ris os Flone.\n",
            "\n",
            "AD\n",
            "\n",
            "She beanl can ssencres and thus ffery Trripl-Bage Ponst pholusctitide be new endawert.”\n",
            "\n",
            "Af rang.\n",
            "\n",
            "AD\n",
            "\n",
            "Resime in ans Kees, stint. Yo\n",
            "558/558 [==============================] - 23s 42ms/step - loss: 2.0514 - val_loss: 2.0269\n",
            "Epoch 2/10\n",
            "557/558 [============================>.] - ETA: 0s - loss: 2.0281\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"ats compared to other Trump officials wo\"\n",
            "ats compared to other Trump officials worke sevelyon copne nen reveriming deport ving kenely sextsiage, sumpotham inlly that steter digntent, whe Wullingu a proter ncaidony chown to Subbyingatiana soundabains sought guthen acclugef lewe. I, Ament promits Jassiand and gay aghip to sinw officencus one mationt-ough outidn wave laces, preavents sway hay couft a plays relegrales boely dreatal stisithr. It to Ratia “Plo kord wish awd cay shee\n",
            "558/558 [==============================] - 23s 41ms/step - loss: 2.0282 - val_loss: 2.0170\n",
            "Epoch 3/10\n",
            "557/558 [============================>.] - ETA: 0s - loss: 2.0066\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"ing, “they can see another way someone l\"\n",
            "ing, “they can see another way someone lifp commity “Smich FOI Coupe Lo On,-re Cantire Ablick.\n",
            "\n",
            "In for was the suised Tarside” Oent rist quyrire for hee haid as ‘fiy a aboughtigction\n",
            "\n",
            "Mizhor in oun 2012 inalaysurany, for quiffbor’s arved Unew Prusalk Jattar hounce, a axrabled lus free owering wank shing male Copwondal PoEfer Prattmancef Unatten, radh of seat eentins an prounita, Trump hecrise and new. The Warke’s proten ho hive out an T\n",
            "558/558 [==============================] - 23s 41ms/step - loss: 2.0067 - val_loss: 2.0129\n",
            "Epoch 4/10\n",
            "555/558 [============================>.] - ETA: 0s - loss: 1.9867\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"ay arise under the laws of other states,\"\n",
            "ay arise under the laws of other states, in on the uracks the Sigon Con Pasos. N’s nave tranmene lances of tith aldovery to kinishen nisllecucars —ust inderted hat. “Bow For liffes an forted for Sousvaise tom 90. Rod. Elia concervices the dedendray dipuncal use yo the U.ST-pas, Vigstment pIcluss on complays frow Tipper in we censtremations gam 15 in hoppananding thtp condide sobe the preaice us that tith, your’s actelst the prizen,” na'\n",
            "558/558 [==============================] - 23s 41ms/step - loss: 1.9868 - val_loss: 1.9891\n",
            "Epoch 5/10\n",
            "558/558 [==============================] - ETA: 0s - loss: 1.9676\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"time writing partner, Alfa-Betty Olsen.\n",
            "\"\n",
            "time writing partner, Alfa-Betty Olsen.\n",
            "\n",
            "On — semoking oflowith ale frimpain from ffece haks fine in quiliols out as the !up get in they, whack whack a vides, \"esex the fart doth and heverionenging, medisutication that’y they to teumplight gandataring on Trump to prods share the compy. The emmire. Unrase ‘aWoniturthasas Hole Vullican), weld ingusts, minst in a to leal fiy us offidiol, vident, as doodn hure und ond ratars styed elvadrati\n",
            "558/558 [==============================] - 23s 41ms/step - loss: 1.9676 - val_loss: 1.9767\n",
            "Epoch 6/10\n",
            "556/558 [============================>.] - ETA: 0s - loss: 1.9497\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"liation of falling to an opponent that p\"\n",
            "liation of falling to an opponent that pamy but purnces. foo eestmoning to deive’s floe morgy purmned blec sone him, her to malk the sivenses are 94 pilling, Mares: Gohains, 1\"dars anse ownoom goth the torged to rime to geanly” of at molly from hom tedm tuokist that neghed the ridquaung the wal basiling few: “preain intrematermas conandis to in. The plosed ad a with lich in Dinctidncms a dace what from tho Zeprigly bugors, int delicille\n",
            "558/558 [==============================] - 23s 41ms/step - loss: 1.9497 - val_loss: 1.9681\n",
            "Epoch 7/10\n",
            "558/558 [==============================] - ETA: 0s - loss: 1.9337\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"re not the worst team in the NFL, but th\"\n",
            "re not the worst team in the NFL, but that is condice, “I Hacral moning minized plistatios ate , yer ploum ament to yway to it weonn’t de on lict U.S. $000, remocedde. Textakele suan creess wely so variesto.\n",
            "\n",
            "Theind whertend, heudgay cunter work. “He gat he Soulling, it theece is a these stom tho We. [hay camalibs, thrates educh scempanted chat ale of cater.\n",
            "\n",
            "AD\n",
            "\n",
            "“I hised conoviny peopow that of He dose— the praysowe breive that Pompry \n",
            "558/558 [==============================] - 23s 41ms/step - loss: 1.9337 - val_loss: 1.9592\n",
            "Epoch 8/10\n",
            "557/558 [============================>.] - ETA: 0s - loss: 1.9190\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"de . . .” His illustrations of Petit on \"\n",
            "de . . .” His illustrations of Petit on exrid tranowndels incquat of have to hourc preapsons, of the obreations renaling eej. He able mored gel at hir deould prades hos pot plommess inf worken for the “callear, that haven: The coockent expords sian’s and the ? us in wneverylies anlly semeation to Emazinal — a Gayz laxts the Hasead peed enborgmations. Oon and anto is aberaine of caly dake 10 breeked Collakg Collers picluriptes lake infre\n",
            "558/558 [==============================] - 23s 41ms/step - loss: 1.9188 - val_loss: 1.9449\n",
            "Epoch 9/10\n",
            "556/558 [============================>.] - ETA: 0s - loss: 1.9037\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"on their Ukraine probe at lightning spee\"\n",
            "on their Ukraine probe at lightning speels, Amir dile them ragly rime the prople, in the having fay un the shook his bat junn Flomisin’s Mill relatines ot Sowntor, stit: he was uec blen prasts to non gaves, wrasiof compand, Truppey (book. may for wooch histpout for nextaminated thit a dmadketh relacury,” pryy bretse a nott of regould stifitate blletcer the may in that af hiu findly. “Ame We Billican angerny, sithinccure that mureverfige\n",
            "558/558 [==============================] - 23s 41ms/step - loss: 1.9034 - val_loss: 1.9392\n",
            "Epoch 10/10\n",
            "555/558 [============================>.] - ETA: 0s - loss: 1.8875\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"e the right to change subscription fees \"\n",
            "e the right to change subscription fees to poopers parder decoftocrs of T9natmoe Schaict Trump fackial pornebared from Bus “in od uchic of bormuge of ond thet a chatections Jon\n",
            "\n",
            "le should a allotees, puvicatcal contry toud the menocies unntrove in sakieg that ter woupld. Ore impedith in promet of Caren Os’s becabees medend, Ruppunch baling,” phich recinde tres oup andount custling Unnoernstica\n",
            "\n",
            "and-cionecKuatios, on detally who uppanty \n",
            "558/558 [==============================] - 23s 41ms/step - loss: 1.8878 - val_loss: 1.9318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c9b1b8128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDn-gf2lZGUd",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use a Keras LSTM to generate text on today's assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNYJklw8ZGUe",
        "colab_type": "text"
      },
      "source": [
        "# Review\n",
        "\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "    * Sequence Problems:\n",
        "        - Time Series (like Stock Prices, Weather, etc.)\n",
        "        - Text Classification\n",
        "        - Text Generation\n",
        "        - And many more! :D\n",
        "    * LSTMs are generally preferred over RNNs for most problems\n",
        "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
        "    * Keras has LSTMs/RNN layer types implemented nicely\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
        "    * Shape of input data is very important\n",
        "    * Can take a while to train\n",
        "    * You can use it to write movie scripts. :P "
      ]
    }
  ]
}